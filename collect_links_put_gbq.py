import os
import json
import re
from datetime import datetime, timedelta
from google.cloud import bigquery
from google.oauth2 import service_account
import pandas as pd
import tempfile


class BigQueryReplayCollector:
    def __init__(self, credentials_path, project_id, dataset_id, table_id,
                 output_dataset_id='amplitude_session_replay'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞ Session Replay ID –∏–∑ BigQuery"""
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.table_id = table_id
        self.output_dataset_id = output_dataset_id
        self.output_table_id = 'session_replay_urls'
        self.full_table_name = f"`{project_id}.{dataset_id}.{table_id}`"
        self.output_table_name = f"{project_id}.{output_dataset_id}.{self.output_table_id}"

        print("üîê –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é...")
        print(f"üìÅ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å credentials –∏–∑: {credentials_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(credentials_path):
            raise FileNotFoundError(f"‚ùå –§–∞–π–ª credentials –Ω–µ –Ω–∞–π–¥–µ–Ω: {credentials_path}")

        # –°–æ–∑–¥–∞–µ–º credentials –∏ client
        try:
            credentials = service_account.Credentials.from_service_account_file(
                credentials_path,
                scopes=["https://www.googleapis.com/auth/bigquery"]
            )

            self.client = bigquery.Client(credentials=credentials, project=project_id)
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ BigQuery —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            print(f"üìä –ò—Å—Ö–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞: {self.full_table_name}")
            print(f"üíæ –¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {self.output_table_name}")

        except Exception as e:
            raise Exception(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è BigQuery client: {e}")

    def create_output_table(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ Session Replay"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            try:
                dataset = self.client.get_dataset(f"{self.project_id}.{self.output_dataset_id}")
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç {self.output_dataset_id} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            except:
                print(f"üìÅ –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç {self.output_dataset_id}...")
                dataset = bigquery.Dataset(f"{self.project_id}.{self.output_dataset_id}")
                dataset.location = "US"
                dataset = self.client.create_dataset(dataset)
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç {self.output_dataset_id} —Å–æ–∑–¥–∞–Ω")

            # –°—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü—ã
            schema = [
                bigquery.SchemaField("record_date", "DATE", mode="REQUIRED"),
                bigquery.SchemaField("session_replay_url", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("collection_datetime", "TIMESTAMP", mode="REQUIRED"),
                bigquery.SchemaField("is_processed", "BOOLEAN", mode="REQUIRED"),
                bigquery.SchemaField("amplitude_id", "INTEGER", mode="NULLABLE"),
                bigquery.SchemaField("session_replay_id", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("duration_seconds", "FLOAT", mode="NULLABLE"),
                bigquery.SchemaField("events_count", "INTEGER", mode="NULLABLE"),
                bigquery.SchemaField("processed_datetime", "TIMESTAMP", mode="NULLABLE"),
                bigquery.SchemaField("screenshots_count", "INTEGER", mode="NULLABLE"),
                bigquery.SchemaField("drive_folder_id", "STRING", mode="NULLABLE"),
            ]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            try:
                table = self.client.get_table(self.output_table_name)
                print(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {self.output_table_id} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                return table
            except:
                print(f"üìä –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É {self.output_table_id}...")
                table_ref = bigquery.Table(self.output_table_name, schema=schema)
                table = self.client.create_table(table_ref)
                print(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {self.output_table_id} —Å–æ–∑–¥–∞–Ω–∞")
                return table

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã: {e}")
            raise

    def test_connection(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        try:
            test_query = f"""
            SELECT COUNT(*) as total_events
            FROM {self.full_table_name}
            WHERE TIMESTAMP_TRUNC(event_time, DAY) >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
            LIMIT 1
            """

            print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ BigQuery...")
            result = self.client.query(test_query).result()

            for row in result:
                print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ! –°–æ–±—ã—Ç–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: {row.total_events:,}")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def get_session_replay_ids_with_duration(self, start_date, end_date, min_duration_seconds=20, amplitude_id=None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Session Replay ID —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ—Å—Å–∏–∏"""

        query = f"""
        WITH session_data AS (
            SELECT 
                amplitude_id,
                REGEXP_EXTRACT(
                    TO_JSON_STRING(event_properties),
                    r'"\[Amplitude\] Session Replay ID":"([^"]+)"'
                ) AS session_replay_id,
                REGEXP_EXTRACT(
                    TO_JSON_STRING(event_properties),
                    r'"\[Amplitude\] Session Replay ID":"[^/]+/(\d+)"'
                ) AS session_start_time_millis,
                SAFE_CAST(REGEXP_EXTRACT(
                    TO_JSON_STRING(event_properties),
                    r'"\[Amplitude\] Session Replay ID":"[^/]+/\d+/(\d+)"'
                ) AS INT64) AS session_duration_ms,
                event_time
            FROM {self.full_table_name}
            WHERE 
                TIMESTAMP_TRUNC(event_time, DAY) >= TIMESTAMP("{start_date}")
                AND TIMESTAMP_TRUNC(event_time, DAY) <= TIMESTAMP("{end_date}")
                AND TO_JSON_STRING(event_properties) LIKE '%Session Replay ID%'
        ),

        session_stats AS (
            SELECT 
                amplitude_id,
                session_replay_id,
                session_start_time_millis,
                session_duration_ms,
                COALESCE(
                    session_duration_ms,
                    CAST((MAX(UNIX_MILLIS(event_time)) - MIN(UNIX_MILLIS(event_time))) AS INT64)
                ) AS calculated_duration_ms,
                COUNT(*) as events_count,
                MIN(event_time) as first_event,
                MAX(event_time) as last_event
            FROM session_data
            WHERE session_replay_id IS NOT NULL 
                AND session_replay_id != ''
                AND session_start_time_millis IS NOT NULL
            GROUP BY amplitude_id, session_replay_id, session_start_time_millis, session_duration_ms
        )

        SELECT 
            amplitude_id,
            session_replay_id,
            session_start_time_millis,
            calculated_duration_ms,
            ROUND(calculated_duration_ms / 1000.0, 1) as duration_seconds,
            events_count,
            first_event,
            last_event,
            DATE(first_event) as record_date
        FROM session_stats
        WHERE calculated_duration_ms >= {min_duration_seconds * 1000}
        """

        if amplitude_id:
            query += f"\n    AND amplitude_id = {amplitude_id}"

        query += """
        ORDER BY amplitude_id, calculated_duration_ms DESC
        """

        print(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
        print(f"‚è±Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏: {min_duration_seconds} —Å–µ–∫—É–Ω–¥")

        try:
            print("‚è≥ –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è Session Replay ID...")
            query_job = self.client.query(query)
            results_list = list(query_job.result())

            print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –∏–∑ BigQuery: {len(results_list)} –∑–∞–ø–∏—Å–µ–π")

            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = []
            for row in results_list:
                data.append({
                    'amplitude_id': row.amplitude_id,
                    'session_replay_id': row.session_replay_id,
                    'session_start_time_millis': row.session_start_time_millis,
                    'duration_seconds': row.duration_seconds,
                    'events_count': row.events_count,
                    'record_date': row.record_date
                })

            return pd.DataFrame(data)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {e}")
            raise

    def format_replay_urls(self, df, project_id="258068",
                           base_url="https://app.amplitude.com/analytics/rn/session-replay"):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataFrame –≤ —Å—Å—ã–ª–∫–∏ –Ω–∞ Session Replay"""

        urls_data = []
        current_time = datetime.now()

        print(f"üîó –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ URL...")

        for index, row in df.iterrows():
            try:
                amplitude_id = row['amplitude_id']
                session_replay_id = row['session_replay_id']
                session_start_time = row['session_start_time_millis']

                # –§–æ—Ä–º–∏—Ä—É–µ–º URL
                url = (
                    f"{base_url}/project/{project_id}/"
                    f"search/amplitude_id%3D{amplitude_id}?"
                    f"sessionReplayId={session_replay_id}&"
                    f"sessionStartTime={session_start_time}"
                )

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è BigQuery
                url_record = {
                    'record_date': row['record_date'].strftime('%Y-%m-%d'),
                    'session_replay_url': url,
                    'collection_datetime': current_time.isoformat(),
                    'is_processed': False,
                    'amplitude_id': int(amplitude_id),
                    'session_replay_id': session_replay_id,
                    'duration_seconds': float(row['duration_seconds']),
                    'events_count': int(row['events_count']),
                    'processed_datetime': None,
                    'screenshots_count': None,
                    'drive_folder_id': None
                }

                urls_data.append(url_record)

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è URL –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {index}: {e}")
                continue

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(urls_data)} –∑–∞–ø–∏—Å–µ–π")
        return urls_data

    def check_existing_dates(self, start_date, end_date):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∫–∏–µ –¥–∞—Ç—ã —É–∂–µ –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ"""
        try:
            check_query = f"""
            SELECT DISTINCT record_date
            FROM `{self.output_table_name}`
            WHERE record_date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY record_date
            """

            result = self.client.query(check_query).result()
            existing_dates = {row.record_date for row in result}

            if existing_dates:
                print(f"üìÖ –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –¥–∞—Ç—ã: {sorted(existing_dates)}")
                return existing_dates
            else:
                print(f"‚úÖ –î–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date} –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Ç")
                return set()

        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã (–≤–æ–∑–º–æ–∂–Ω–æ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç): {e}")
            return set()

    def filter_new_data(self, urls_data, existing_dates):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞—Ç—ã"""
        if not existing_dates:
            return urls_data

        original_count = len(urls_data)

        filtered_data = []
        for record in urls_data:
            record_date_str = record['record_date']
            record_date = datetime.strptime(record_date_str, '%Y-%m-%d').date()

            if record_date not in existing_dates:
                filtered_data.append(record)

        filtered_count = len(filtered_data)
        skipped_count = original_count - filtered_count

        if skipped_count > 0:
            print(f"üîÑ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {skipped_count} –∑–∞–ø–∏—Å–µ–π (–¥–∞—Ç—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç)")
            print(f"‚úÖ –û—Å—Ç–∞–ª–æ—Å—å {filtered_count} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")

        return filtered_data

    def save_urls_to_bigquery(self, urls_data, start_date, end_date):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ URL –≤ BigQuery —Ç–∞–±–ª–∏—Ü—É —á–µ—Ä–µ–∑ batch load (CSV)"""
        if not urls_data:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            self.create_output_table()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã
            existing_dates = self.check_existing_dates(start_date, end_date)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞—Ç—ã
            urls_data = self.filter_new_data(urls_data, existing_dates)

            if not urls_data:
                print("‚ÑπÔ∏è –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ —Ç–∞–±–ª–∏—Ü–µ")
                return

            print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(urls_data)} –∑–∞–ø–∏—Å–µ–π –≤ BigQuery...")

            # –°–æ–∑–¥–∞–µ–º DataFrame
            df = pd.DataFrame(urls_data)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π CSV —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(mode='w', suffix=".csv", delete=False) as temp_file:
                df.to_csv(temp_file.name, index=False, header=False)
                temp_file_path = temp_file.name

            try:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
                table_ref = self.client.dataset(self.output_dataset_id).table(self.output_table_id)
                job_config = bigquery.LoadJobConfig()
                job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND
                job_config.source_format = bigquery.SourceFormat.CSV
                job_config.autodetect = False

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –≤ BigQuery
                with open(temp_file_path, "rb") as source_file:
                    job = self.client.load_table_from_file(source_file, table_ref, job_config=job_config)

                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
                print("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ BigQuery...")
                job.result()

                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls_data)} –∑–∞–ø–∏—Å–µ–π –≤ BigQuery")

            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ BigQuery: {e}")
            raise


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö –°–ë–û–†–©–ò–ö–ê SESSION REPLAY ID")
    print("=" * 50)

    CONFIG = {
        'credentials_path': '/Users/avsimkin/PycharmProjects/session_replay_ai/venv/bigquery-credentials.json',
        'project_id': 'codellon-dwh',
        'dataset_id': 'amplitude',
        'table_id': 'EVENTS_258068',
        'output_dataset_id': 'amplitude_session_replay',
        'amplitude_project_id': '258068',
        'min_duration_seconds': 20
    }

    days_back = 2
    end_date = datetime.now().date()
    start_date = end_date - timedelta(days=days_back)

    print(f"üìÖ –ü–µ—Ä–∏–æ–¥ —Å–±–æ—Ä–∞: {start_date} - {end_date}")
    print(f"‚è±Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {CONFIG['min_duration_seconds']} —Å–µ–∫—É–Ω–¥")

    try:
        collector = BigQueryReplayCollector(
            credentials_path=CONFIG['credentials_path'],
            project_id=CONFIG['project_id'],
            dataset_id=CONFIG['dataset_id'],
            table_id=CONFIG['table_id'],
            output_dataset_id=CONFIG['output_dataset_id']
        )

        if not collector.test_connection():
            print("‚ùå –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –Ω–µ –ø—Ä–æ—à–µ–ª. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É.")
            return

        print(f"üîç –°–æ–±–∏—Ä–∞–µ–º Session Replay ID...")
        df = collector.get_session_replay_ids_with_duration(
            start_date=start_date.strftime('%Y-%m-%d'),
            end_date=end_date.strftime('%Y-%m-%d'),
            min_duration_seconds=CONFIG['min_duration_seconds']
        )

        if df.empty:
            print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏")
            return

        print(f"üîó –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º URL...")
        urls_data = collector.format_replay_urls(
            df,
            project_id=CONFIG['amplitude_project_id']
        )

        if not urls_data:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ URL")
            return

        collector.save_urls_to_bigquery(urls_data, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))

        print(f"\nüéâ –ì–û–¢–û–í–û!")
        print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(urls_data)} Session Replay URL")
        print(f"üíæ –¢–∞–±–ª–∏—Ü–∞: {CONFIG['project_id']}.{CONFIG['output_dataset_id']}.session_replay_urls")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()