import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from collections import Counter
import nltk
import re
import os
import sys
from datetime import datetime
from typing import Callable, Optional

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è Google Cloud
from google.cloud import bigquery
from google.oauth2 import service_account

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', '/etc/secrets/bigquery-credentials.json')
        BQ_PROJECT_ID = os.environ.get('BQ_PROJECT_ID', 'codellon-dwh')
        BQ_DATASET_ID = os.environ.get('BQ_DATASET_ID', 'amplitude_session_replay')
        BQ_CLUSTERING_TABLE = os.environ.get('BQ_CLUSTERING_TABLE', 'replay_text_complete')
    settings = MockSettings()

class ClusteringAnalysisProcessor:
    def __init__(self, status_callback: Optional[Callable[[str, int], None]] = None):
        self.status_callback = status_callback
        self.credentials_path = settings.GOOGLE_APPLICATION_CREDENTIALS
        self.bq_project_id = settings.BQ_PROJECT_ID
        self.bq_dataset_id = settings.BQ_DATASET_ID
        self.bq_table = settings.BQ_CLUSTERING_TABLE
        
        # NLTK data path –¥–ª—è Render
        nltk_data_path = os.environ.get('NLTK_DATA', '/opt/render/project/src/nltk_data')
        if os.path.exists(nltk_data_path):
            nltk.data.path.append(nltk_data_path)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã
        self.start_time = None
        self.total_processed = 0
        self.total_successful = 0
        self.total_failed = 0
        
        self._update_status("üîê –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...", 1)
        self._init_clients()
        self._setup_nltk()

    def _update_status(self, details: str, progress: int):
        if self.status_callback: 
            self.status_callback(details, progress)
        if progress != -1: 
            print(f"[{progress}%] {details}")
        else:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] {details}")

    def _init_clients(self):
        try:
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path,
                scopes=["https://www.googleapis.com/auth/bigquery"]
            )
            self.bq_client = bigquery.Client(credentials=credentials, project=self.bq_project_id)
            self._update_status("‚úÖ BigQuery –ø–æ–¥–∫–ª—é—á–µ–Ω", 5)
        except Exception as e:
            raise Exception(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ BigQuery: {e}")

    def _setup_nltk(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–∞—á–∞—Ç—å stopwords –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            try:
                from nltk.corpus import stopwords
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
                stopwords.words("russian")
                self._update_status("‚úÖ NLTK stopwords –¥–æ—Å—Ç—É–ø–Ω—ã", -1)
            except:
                self._update_status("‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–µ–º NLTK stopwords...", -1)
                nltk.download('stopwords', quiet=True)
                from nltk.corpus import stopwords
                
            self.russian_stopwords = set(stopwords.words("russian"))
            self.extra_stopwords = set([
                'user', 'session', 'began', 'application', 'the', 'and', 'to', 'a', 'in', 'with',
                'click', 'entered', 'selected', 'form', 'page'
            ])
            
        except Exception as e:
            self._update_status(f"‚ö†Ô∏è NLTK –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ stopwords: {e}", -1)
            # Fallback —Å–ø–∏—Å–æ–∫ stopwords
            self.russian_stopwords = set(['–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–Ω–µ', '–æ—Ç', '–¥–æ', '–∏–∑'])
            self.extra_stopwords = set([
                'user', 'session', 'began', 'application', 'the', 'and', 'to', 'a', 'in', 'with',
                'click', 'entered', 'selected', 'form', 'page'
            ])

    def get_rows_without_clusters(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–∑ BigQuery, –≥–¥–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Å—Ç—ã–µ"""
        table_id = f"{self.bq_project_id}.{self.bq_dataset_id}.{self.bq_table}"
        query = f"""
        SELECT * FROM `{table_id}`
        WHERE advanced_cluster IS NULL OR cluster_description IS NULL
        """
        
        self._update_status("üîç –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ BigQuery...", 10)
        
        try:
            df = self.bq_client.query(query).to_dataframe()
            self._update_status(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(df)}", 15)
            return df
        except Exception as e:
            self._update_status(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}", -1)
            raise

    def extract_features_advanced(self, row):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–∏"""
        summary = str(row.get('summary', '')).lower()
        actions = str(row.get('actions', '')).lower()
        sentiment = str(row.get('sentiment', '')).lower()
        combined_text = f"{summary} {actions} {sentiment}"
        
        features = {}
        
        # –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['navigation'] = int(any(word in combined_text for word in [
            'main page', 'navigate', 'opened', 'clicked', 'menu', 'link'
        ]))
        features['payment'] = int(any(word in combined_text for word in [
            'deposit', 'payment', 'cash', 'money', 'balance', 'refill', 'pay'
        ]))
        features['betting'] = int(any(word in combined_text for word in [
            'bet', 'betting', 'stake', 'wager', 'place', 'odds'
        ]))
        features['gaming'] = int(any(word in combined_text for word in [
            'game', 'gaming', 'stream', 'live', 'match', 'sport'
        ]))
        features['auth'] = int(any(word in combined_text for word in [
            'login', 'register', 'authorization', 'auth', 'sign'
        ]))
        features['mobile'] = int(any(word in combined_text for word in [
            'mobile', 'app', 'download', 'apk', 'application'
        ]))
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['tech_error'] = int(any(word in combined_text for word in [
            'error', 'fail', 'invalid', 'refused', 'not working', 'timeout'
        ]))
        features['ux_issue'] = int(any(word in combined_text for word in [
            'confused', 'unclear', 'difficult', 'complicated', 'lost'
        ]))
        features['performance'] = int(any(word in combined_text for word in [
            'slow', 'loading', 'lag', 'freeze', 'stuck'
        ]))
        features['successful'] = int(any(word in combined_text for word in [
            'successful', 'completed', 'finished', 'achieved'
        ]))
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        event_total = row.get('event_total', 0)
        try:
            features['event_count'] = min(int(event_total) / 20.0, 1.0)
        except:
            features['event_count'] = 0
        
        # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏
        session_length = str(row.get('session_length', '')).lower()
        features['long_session'] = 1 if 'h' in session_length else 0
        features['medium_session'] = 1 if 'm' in session_length and 'h' not in session_length else 0
        features['short_session'] = 1 if features['long_session'] == 0 and features['medium_session'] == 0 else 0
        
        return features

    def smart_categorize(self, row, features):
        """–£–º–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if features['payment'] and features['tech_error']:
            return '–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–µ–ø–æ–∑–∏—Ç–∞–º–∏/–ø–ª–∞—Ç–µ–∂–∞–º–∏'
        if features['mobile'] and (features['tech_error'] or 'download' in str(row['summary']).lower()):
            return '–ü—Ä–æ–±–ª–µ–º—ã —Å –º–æ–±–∏–ª—å–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º'
        if features['auth'] and features['tech_error']:
            return '–ü—Ä–æ–±–ª–µ–º—ã —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π'
        if features['betting'] and features['tech_error']:
            return '–ü—Ä–æ–±–ª–µ–º—ã —Å–æ —Å—Ç–∞–≤–∫–∞–º–∏'
        
        # –£—Å–ø–µ—à–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if features['payment'] and features['successful']:
            return '–£—Å–ø–µ—à–Ω—ã–µ –¥–µ–ø–æ–∑–∏—Ç—ã'
        if features['betting'] and features['successful']:
            return '–£—Å–ø–µ—à–Ω—ã–µ —Å—Ç–∞–≤–∫–∏'
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if features['gaming'] or features['betting']:
            return '–ò–≥—Ä–æ–≤–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
        if features['navigation'] and features['event_count'] > 0.5:
            return '–ê–∫—Ç–∏–≤–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è'
        if features['long_session'] and features['event_count'] > 0.3:
            return '–î–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏'
        if features['short_session'] and features['event_count'] < 0.2:
            return '–ö–æ—Ä–æ—Ç–∫–∏–µ/–Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏'
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        if features['mobile']:
            return '–ú–æ–±–∏–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
        if features['performance']:
            return '–ü—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'
        if features['ux_issue']:
            return 'UX –ø—Ä–æ–±–ª–µ–º—ã'
        
        # –ü–æ sentiment
        sentiment_label = row.get('sentiment_label', '')
        if sentiment_label == 'negative':
            return '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç'
        elif sentiment_label == 'positive' and features['successful']:
            return '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç'
        
        return '–û–±—ã—á–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'

    def extract_sentiment(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ sentiment –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text = str(text).lower()
        if 'positive' in text:
            return 'positive'
        elif 'negative' in text:
            return 'negative'
        elif 'neutral' in text:
            return 'neutral'
        return 'unknown'

    def has_problem_advanced(self, row):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –ø—Ä–æ–±–ª–µ–º—ã"""
        if row['sentiment_label'] == 'negative':
            return 1
        
        combined_text = str(row['summary']) + ' ' + str(row['actions'])
        problem_indicators = [
            'error', 'fail', 'invalid', 'refused', 'not working', 'timeout',
            'unable', 'cannot', 'problem', 'issue', 'difficulty'
        ]
        
        if any(indicator in combined_text.lower() for indicator in problem_indicators):
            return 1
        
        if row['sentiment_label'] == 'neutral':
            negative_actions = ['did not', 'failed to', 'unsuccessful', 'incomplete']
            if any(action in combined_text.lower() for action in negative_actions):
                return 1
        
        return 0

    def detect_problem_source_advanced(self, row):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã"""
        combined_text = (str(row['summary']) + ' ' + str(row['actions'])).lower()
        
        if any(w in combined_text for w in ['deposit', 'payment', 'cash', 'money', 'balance', 'refill']):
            return '–¥–µ–ø–æ–∑–∏—Ç'
        elif any(w in combined_text for w in ['mobile', 'app', 'download', 'apk']):
            return '–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ'
        elif any(w in combined_text for w in ['bet', 'betting', 'stake', 'wager']):
            return '—Å—Ç–∞–≤–∫–∏'
        elif any(w in combined_text for w in ['login', 'register', 'auth', 'sign']):
            return '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è/–ª–æ–≥–∏–Ω'
        elif any(w in combined_text for w in ['game', 'gaming', 'stream']):
            return '–∏–≥—Ä–∞'
        elif any(w in combined_text for w in ['navigation', 'menu', 'page']):
            return '–Ω–∞–≤–∏–≥–∞—Ü–∏—è'
        else:
            return '–ø—Ä–æ—á–µ–µ'

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è TF-IDF"""
        text = text.lower()
        words = [w for w in text.split() if w.isalpha() and len(w) > 2 and
                 w not in self.russian_stopwords and w not in self.extra_stopwords]
        return " ".join(words)

    def update_session_in_bq(self, session_data):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏ –≤ BigQuery"""
        table_id = f"{self.bq_project_id}.{self.bq_dataset_id}.{self.bq_table}"
        
        update_query = f"""
        UPDATE `{table_id}`
        SET
            advanced_cluster = @advanced_cluster,
            cluster_description = @cluster_description,
            smart_category = @smart_category,
            has_problem = @has_problem,
            problem_source = @problem_source,
            sentiment_label = @sentiment_label
        WHERE session_id = @session_id
        """
        
        try:
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("advanced_cluster", "INT64", int(session_data['advanced_cluster'])),
                    bigquery.ScalarQueryParameter("cluster_description", "STRING", str(session_data['cluster_description'])),
                    bigquery.ScalarQueryParameter("smart_category", "STRING", str(session_data['smart_category'])),
                    bigquery.ScalarQueryParameter("has_problem", "INT64", int(session_data['has_problem'])),
                    bigquery.ScalarQueryParameter("problem_source", "STRING", str(session_data['problem_source'])),
                    bigquery.ScalarQueryParameter("sentiment_label", "STRING", str(session_data['sentiment_label'])),
                    bigquery.ScalarQueryParameter("session_id", "STRING", str(session_data['session_id'])),
                ]
            )
            self.bq_client.query(update_query, job_config=job_config).result()
            return True
        except Exception as e:
            self._update_status(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è session_id {session_data['session_id']}: {e}", -1)
            return False

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞"""
        self.start_time = datetime.now()
        
        self._update_status("üîÑ –ó–ê–ü–£–°–ö –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –ò –ê–ù–ê–õ–ò–ó–ê", 20)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        df = self.get_rows_without_clusters()
        
        if df.empty:
            self._update_status("‚úÖ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏!", 100)
            return {"status": "no_data", "message": "–í—Å–µ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω—ã"}

        self._update_status(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ {len(df)} –∑–∞–ø–∏—Å–µ–π", 25)
        
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self._update_status("üîß –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏...", 30)
            features_list = []
            for _, row in df.iterrows():
                features = self.extract_features_advanced(row)
                features_list.append(features)
            features_df = pd.DataFrame(features_list)

            # Smart –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            self._update_status("üè∑Ô∏è –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—é...", 40)
            smart_categories = []
            for i, row in df.iterrows():
                category = self.smart_categorize(row, features_list[i])
                smart_categories.append(category)
            df['smart_category'] = smart_categories

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ sentiment
            self._update_status("üòä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º sentiment...", 45)
            df['sentiment_label'] = df['sentiment'].apply(self.extract_sentiment)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
            self._update_status("üîç –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã...", 50)
            df['has_problem'] = df.apply(self.has_problem_advanced, axis=1)
            df['problem_source'] = df.apply(self.detect_problem_source_advanced, axis=1)

            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            self._update_status("üéØ –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...", 60)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
            texts = (
                df['summary'].fillna('') + ' ' +
                df['sentiment'].fillna('') + ' ' +
                df['actions'].fillna('')
            ).values

            texts_clean = [self.clean_text(t) for t in texts]
            
            # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
            vectorizer = TfidfVectorizer(max_features=1000, min_df=1, max_df=0.8)
            text_features = vectorizer.fit_transform(texts_clean)
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            scaler = StandardScaler()
            numeric_features = scaler.fit_transform(features_df.values)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            from scipy.sparse import hstack
            combined_features = hstack([text_features, numeric_features])
            
            # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            n_clusters = min(8, len(df) // 2) if len(df) > 1 else 1
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(combined_features.toarray())
            df['advanced_cluster'] = cluster_labels

            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            self._update_status("üìù –°–æ–∑–¥–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 70)
            cluster_descriptions = []
            for cluster_id in sorted(set(cluster_labels)):
                cluster_mask = df['advanced_cluster'] == cluster_id
                cluster_data = df[cluster_mask]
                cluster_categories = cluster_data['smart_category'].value_counts()
                top_category = cluster_categories.index[0] if len(cluster_categories) > 0 else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                description = f"{top_category}"
                if len(cluster_categories) > 1:
                    description += f" + {cluster_categories.index[1]}"
                cluster_descriptions.append(description)
            
            cluster_desc_map = {i: desc for i, desc in enumerate(cluster_descriptions)}
            df['cluster_description'] = df['advanced_cluster'].map(cluster_desc_map)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ BigQuery
            self._update_status("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ BigQuery...", 80)
            
            for i, (_, row) in enumerate(df.iterrows()):
                progress = 80 + int((i / len(df)) * 15)
                self._update_status(f"üíæ –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å {i+1}/{len(df)}", progress)
                
                success = self.update_session_in_bq(row.to_dict())
                if success:
                    self.total_successful += 1
                else:
                    self.total_failed += 1
                
                self.total_processed += 1

        except Exception as e:
            self._update_status(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}", -1)
            raise

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = datetime.now() - self.start_time
        result = {
            "status": "completed",
            "total_processed": self.total_processed,
            "total_successful": self.total_successful,
            "total_failed": self.total_failed,
            "success_rate": f"{(self.total_successful/self.total_processed*100):.1f}%" if self.total_processed > 0 else "0%",
            "clusters_created": len(set(cluster_labels)) if 'cluster_labels' in locals() else 0,
            "total_time_minutes": round(total_time.total_seconds() / 60, 1)
        }

        self._update_status(f"üèÅ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!", 100)
        self._update_status(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_processed}, –£—Å–ø–µ—à–Ω–æ: {self.total_successful}, –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {result['clusters_created']}", 100)
        
        return result


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    try:
        def console_status_callback(details: str, progress: int):
            if progress != -1:
                print(f"[{progress}%] {details}")
            else:
                print(f"[INFO] {details}")

        processor = ClusteringAnalysisProcessor(status_callback=console_status_callback)
        result = processor.run()
        
        print(f"\nüèÅ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return result

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "error": str(e)}


if __name__ == "__main__":
    main()