# Ð˜Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°: replay_screenshots.py
import json
import os
import time
import hashlib
import random
import sys
from datetime import datetime
from playwright.sync_api import sync_playwright, Error as PlaywrightError, TimeoutError as PlaywrightTimeoutError
import tempfile
import shutil
from typing import Callable, Optional
import zipfile
import multiprocessing
import queue

# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Google API
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.cloud import bigquery

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ðº ÐºÐ¾Ñ€Ð½ÑŽ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð´Ð»Ñ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð° config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', '/etc/secrets/bigquery-credentials.json')
        GDRIVE_FOLDER_ID = os.environ.get('GDRIVE_FOLDER_ID', '1K8cbFU2gYpvP3PiHwOOHS1KREqdj6fQX')
        BQ_PROJECT_ID = os.environ.get('BQ_PROJECT_ID', 'codellon-dwh')
        BQ_DATASET_ID = os.environ.get('BQ_DATASET_ID', 'amplitude_session_replay')
        BQ_TABLE_ID = os.environ.get('BQ_TABLE_ID', 'session_replay_urls')
    settings = MockSettings()

PROCESS_TIMEOUT_PER_URL = 240

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
]

def sanitize_cookies(cookies):
    if not cookies: return []
    valid_same_site_values = {"Strict", "Lax", "None"}
    sanitized_cookies = []
    for cookie in cookies:
        if cookie.get('sameSite') not in valid_same_site_values:
            original_value = cookie.get('sameSite', 'ÐšÐ›Ð®Ð§ ÐžÐ¢Ð¡Ð£Ð¢Ð¡Ð¢Ð’ÐžÐ’ÐÐ›')
            print(f"âš ï¸ Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð½ÐµÐ²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹/Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ sameSite='{original_value}' Ð½Ð° 'Lax' Ð´Ð»Ñ ÐºÑƒÐºÐ¸: {cookie.get('name')}")
            cookie['sameSite'] = 'Lax'
        sanitized_cookies.append(cookie)
    return sanitized_cookies

def worker_process_url(collector_config: dict, url_data: dict, result_queue: multiprocessing.Queue):
    try:
        collector = RenderScreenshotCollector(config_override=collector_config)
        sanitized_cookies = sanitize_cookies(collector.cookies)

        with sync_playwright() as p:
            browser_args = [
                '--no-proxy-server', '--disable-proxy-config-service', '--no-sandbox',
                '--disable-setuid-sandbox', '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage', '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
            browser = p.chromium.launch(headless=True, args=browser_args)
            context = browser.new_context(
                user_agent=random.choice(USER_AGENTS), viewport={'width': 1366, 'height': 768},
                locale='en-US', timezone_id='America/New_York'
            )
            context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
            context.add_cookies(sanitized_cookies)
            page = context.new_page()

            success, _ = collector.process_single_url(page, url_data)
            collector.mark_url_as_processed(url_data['url'], success)
            result_queue.put(success)

            page.close()
            context.close()
            browser.close()
    except Exception as e:
        print(f"âŒ ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ð´Ð¾Ñ‡ÐµÑ€Ð½ÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ Ð´Ð»Ñ URL {url_data.get('url')}: {e}")
        import traceback
        traceback.print_exc()
        result_queue.put(False)


class RenderScreenshotCollector:
    def __init__(self, status_callback: Optional[Callable[[str, int], None]] = None, config_override: Optional[dict] = None):
        if config_override:
            self.credentials_path = config_override["credentials_path"]
            self.gdrive_folder_id = config_override["gdrive_folder_id"]
            self.bq_project_id = config_override["bq_project_id"]
            self.bq_dataset_id = config_override["bq_dataset_id"]
            self.bq_table_id = config_override["bq_table_id"]
            self.min_duration_seconds = config_override["min_duration_seconds"]
            self.cookies_path = config_override["cookies_path"]
            self.status_callback = None
            self.cookies = self._load_cookies_from_secret_file(verbose=False)
        else:
            self.status_callback = status_callback
            self.cookies_path = "/etc/secrets/cookies.json"
            self.credentials_path = settings.GOOGLE_APPLICATION_CREDENTIALS
            self.gdrive_folder_id = settings.GDRIVE_FOLDER_ID
            self.bq_project_id = settings.BQ_PROJECT_ID
            self.bq_dataset_id = settings.BQ_DATASET_ID
            self.bq_table_id = settings.BQ_TABLE_ID
            self.batch_size = int(os.environ.get('BATCH_SIZE', '50'))
            self.pause_between_batches = int(os.environ.get('PAUSE_BETWEEN_BATCHES', '300'))
            self.max_runtime_hours = int(os.environ.get('MAX_RUNTIME_HOURS', '18'))
            self.min_duration_seconds = int(os.environ.get('MIN_DURATION_SECONDS', '20'))
            self.start_time = None
            self.total_processed, self.total_successful, self.total_failed, self.total_timeouts, self.batches_completed = 0, 0, 0, 0, 0
            self._update_status("ðŸ” ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ...", 1)
            self.cookies = self._load_cookies_from_secret_file()
        
        self.full_table_name = f"`{self.bq_project_id}.{self.bq_dataset_id}.{self.bq_table_id}`"
        self._init_bigquery()
        self._init_google_drive()

    def _update_status(self, details: str, progress: int):
        if self.status_callback: self.status_callback(details, progress)
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {details}")

    def _load_cookies_from_secret_file(self, verbose=True):
        if verbose: self._update_status(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° cookies Ð¸Ð· {self.cookies_path}...", 2)
        if not os.path.exists(self.cookies_path):
            if verbose: self._update_status(f"âŒ Ð¤Ð°Ð¹Ð» cookies Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!", 2)
            return []
        try:
            with open(self.cookies_path, 'r') as f: cookies = json.load(f)
            if verbose: self._update_status(f"âœ… Cookies Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ ({len(cookies)} ÑˆÑ‚).", 3)
            return cookies
        except Exception as e:
            if verbose: self._update_status(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ cookies: {e}", 3)
            return []

    def _init_bigquery(self):
        try:
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path, scopes=["https://www.googleapis.com/auth/bigquery"])
            self.bq_client = bigquery.Client(credentials=credentials, project=self.bq_project_id)
        except Exception as e:
            raise Exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº BigQuery: {e}")

    def _init_google_drive(self):
        try:
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path, scopes=['https://www.googleapis.com/auth/drive'])
            self.drive_service = build('drive', 'v3', credentials=credentials)
        except Exception as e:
            raise Exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº Google Drive: {e}")

    def get_unprocessed_urls(self, limit=None):
        query = f"SELECT session_replay_url, amplitude_id, session_replay_id, duration_seconds, events_count, record_date FROM {self.full_table_name} WHERE is_processed = FALSE AND duration_seconds >= {self.min_duration_seconds} ORDER BY record_date DESC"
        if limit: query += f"\nLIMIT {limit}"
        try:
            result = self.bq_client.query(query).result()
            urls_data = []
            for row in result:
                urls_data.append({
                    'url': row.session_replay_url, 'amplitude_id': row.amplitude_id,
                    'session_replay_id': row.session_replay_id, 'duration_seconds': row.duration_seconds,
                    'events_count': row.events_count,
                    'record_date': row.record_date.strftime('%Y-%m-%d') if hasattr(row.record_date, 'strftime') else str(row.record_date)
                })
            return urls_data
        except Exception as e:
            self._update_status(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ URL: {e}", -1)
            raise

    def mark_url_as_processed(self, url, success=True):
        try:
            update_query = f"UPDATE {self.full_table_name} SET is_processed = TRUE WHERE session_replay_url = @url"
            job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter("url", "STRING", url)])
            self.bq_client.query(update_query, job_config=job_config).result()
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° URL {url}: {e}")

    def login_and_update_cookies(self, page):
        print("âš ï¸ ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð²Ñ…Ð¾Ð´Ð°. ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸...")
        login = os.environ.get('AMPLITUDE_LOGIN')
        password = os.environ.get('AMPLITUDE_PASSWORD')
        if not login or not password:
            print("âŒ ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ AMPLITUDE_LOGIN Ð¸/Ð¸Ð»Ð¸ AMPLITUDE_PASSWORD Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹!")
            return False
        try:
            page.goto("https://app.amplitude.com/login", timeout=60000)
            print(f"    Ð’Ð²Ð¾Ð´Ð¸Ð¼ Ð»Ð¾Ð³Ð¸Ð½...")
            page.fill('input[name="username"]', login)
            page.click('button[type="submit"]')
            print("    Ð’Ð²Ð¾Ð´Ð¸Ð¼ Ð¿Ð°Ñ€Ð¾Ð»ÑŒ...")
            password_input = page.wait_for_selector('input[name="password"]', timeout=15000)
            password_input.fill(password)
            page.click('button[type="submit"]')
            print("    ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ Ð²Ñ…Ð¾Ð´Ð°...")
            page.wait_for_url(lambda url: "login" not in url, timeout=60000)
            page.wait_for_selector("nav", timeout=30000)
            print("âœ… ÐÐ²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÑˆÐ»Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")
            print("    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ cookies...")
            new_cookies = page.context.cookies()
            with open(self.cookies_path, 'w') as f:
                json.dump(new_cookies, f)
            print("âœ… Cookies ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹!")
            self.cookies = new_cookies
            return True
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")
            try:
                page.screenshot(path="login_error_screenshot.png", full_page=True)
                print("    Ð¡ÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð² login_error_screenshot.png")
            except: pass
            return False
    
    def get_session_id_from_url(self, url):
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        if "sessionReplayId=" in url:
            parts = url.split("sessionReplayId=")[1].split("&")[0].split("/")
            session_replay_id = parts[0]
            session_start_time = parts[1] if len(parts) > 1 else "unknown"
            return f"{session_replay_id}_{session_start_time}_{url_hash}"
        return f"no_session_id_{url_hash}"

    def screenshot_by_title(self, page, block_title, session_id, base_dir):
        print(f"ðŸ” Ð˜Ñ‰ÐµÐ¼ Ð±Ð»Ð¾Ðº '{block_title}'...")
        el = page.query_selector(f'h4:has-text("{block_title}")')
        if not el:
             print(f"âŒ Ð‘Ð»Ð¾Ðº '{block_title}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
             return None
        try:
            img_path = os.path.join(base_dir, f"{session_id}_{block_title.lower()}.png")
            el.screenshot(path=img_path)
            print(f"âœ… {block_title} ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½")
            return img_path
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð° {block_title}: {e}")
            return None

    def create_session_folder_structure(self, session_id, screenshots, url_data):
        session_dir = tempfile.mkdtemp(prefix=f"session_folder_{session_id}_")
        for screenshot_path in screenshots:
            if screenshot_path and os.path.exists(screenshot_path):
                shutil.copy2(screenshot_path, session_dir)
        metadata = {
            "session_id": session_id, "url": url_data['url'], "amplitude_id": url_data['amplitude_id'],
            # ... Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        }
        with open(os.path.join(session_dir, "metadata.json"), 'w') as f:
            json.dump(metadata, f, indent=2)
        return session_dir

    def upload_to_google_drive(self, file_path, filename, folder_id):
        try:
            media = MediaFileUpload(file_path, resumable=True)
            file = self.drive_service.files().create(body={'name': filename, 'parents': [folder_id]}, media_body=media, fields='id, name, webViewLink').execute()
            return file
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² Google Drive: {e}")
            return None

    def create_and_upload_session_archive(self, session_dir, session_id, is_failure=False):
        try:
            prefix = "FAILURE" if is_failure else "session_replay"
            archive_name = f"{prefix}_{session_id}_{int(time.time())}.zip"
            archive_path = shutil.make_archive(archive_name.replace('.zip',''), 'zip', session_dir)
            print(f"ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð°Ñ€Ñ…Ð¸Ð²: {archive_name}")
            uploaded_file = self.upload_to_google_drive(archive_path, archive_name, self.gdrive_folder_id)
            if uploaded_file: print(f"â˜ï¸ ÐÑ€Ñ…Ð¸Ð² Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð² Google Drive")
            return uploaded_file
        finally:
            shutil.rmtree(session_dir, ignore_errors=True)
            if 'archive_path' in locals() and os.path.exists(archive_path):
                os.remove(archive_path)

def process_single_url(self, page, url_data):
        url = url_data['url']
        session_id = self.get_session_id_from_url(url)
        temp_screenshots_dir = tempfile.mkdtemp(prefix=f"screenshots_{session_id}_")
        REQUIRED_BLOCKS = ['userinfo', 'summary', 'sentiment']
        screenshot_paths = []
        
        try:
            print(f"â–¶ï¸ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÐµÑÑÐ¸ÑŽ: {session_id}")
            
            # --- Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð• 1: Ð‘ÐžÐ›Ð•Ð• ÐÐÐ”Ð•Ð–ÐÐžÐ• ÐžÐ–Ð˜Ð”ÐÐÐ˜Ð• Ð—ÐÐ“Ð Ð£Ð—ÐšÐ˜ ---
            # Ð’Ð¼ÐµÑÑ‚Ð¾ domcontentloaded Ð¶Ð´ÐµÐ¼, Ð¿Ð¾ÐºÐ° ÑÐµÑ‚ÐµÐ²Ð°Ñ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð½Ðµ Ð¿Ñ€ÐµÐºÑ€Ð°Ñ‚Ð¸Ñ‚ÑÑ
            page.goto(url, timeout=90000, wait_until='networkidle')
            print("    Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°, Ð´Ð°ÐµÐ¼ 5-10 ÑÐµÐºÑƒÐ½Ð´ Ð½Ð° Ð¿Ñ€Ð¾Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°...")
            time.sleep(random.uniform(5, 10))

            if "/login" in page.url:
                login_successful = self.login_and_update_cookies(page)
                if not login_successful: return False, []
                print(f"    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ÑÑ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÑÑ‹Ð»ÐºÐµ...")
                page.goto(url, timeout=90000, wait_until='networkidle')
                time.sleep(random.uniform(5, 10))

            try:
                # --- Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð• 2: Ð£Ð’Ð•Ð›Ð˜Ð§Ð˜Ð’ÐÐ•Ðœ Ð¢ÐÐ™ÐœÐÐ£Ð¢ Ð˜ ÐÐÐ”Ð•Ð–ÐÐžÐ¡Ð¢Ð¬ ÐšÐ›Ð˜ÐšÐ ---
                print("    Ð˜Ñ‰ÐµÐ¼ Ð²ÐºÐ»Ð°Ð´ÐºÑƒ 'Summary'...")
                # Ð”Ð°ÐµÐ¼ Ð´Ð¾ 45 ÑÐµÐºÑƒÐ½Ð´ Ð½Ð° Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¾Ð³Ð¾ Ð²Ð°Ð¶Ð½Ð¾Ð³Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
                summary_tab = page.wait_for_selector("text=Summary", timeout=45000)
                # Ð˜Ð½Ð¾Ð³Ð´Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÐºÐ»Ð¸Ðº Ð½Ðµ ÑÑ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð¾Ð»ÐµÐµ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´
                summary_tab.click(force=True, timeout=5000)
                print("    ÐšÐ»Ð¸Ðº Ð½Ð° 'Summary' Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½.")
                
                # --- Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð• 3: Ð–Ð”Ð•Ðœ ÐŸÐžÐ¯Ð’Ð›Ð•ÐÐ˜Ð¯ ÐšÐžÐÐšÐ Ð•Ð¢ÐÐžÐ“Ðž Ð­Ð›Ð•ÐœÐ•ÐÐ¢Ð ÐŸÐžÐ¡Ð›Ð• ÐšÐ›Ð˜ÐšÐ ---
                print("    ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð²Ð¾ Ð²ÐºÐ»Ð°Ð´ÐºÐµ...")
                # Ð–Ð´ÐµÐ¼ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ñ‚Ð¾Ñ‚ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½Ðµ ÑƒÑÐ¿ÐµÐ²Ð°Ð» Ð¿Ñ€Ð¾Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒÑÑ
                summary_el = page.wait_for_selector('p.ltext-_uoww22', timeout=45000)
                print("    ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ 'Summary' Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½.")
                time.sleep(random.uniform(2, 4)) # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ð°ÑƒÐ·Ð° Ð¿Ð¾ÑÐ»Ðµ ÐºÐ»Ð¸ÐºÐ°

            except PlaywrightTimeoutError as e:
                print(f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¸Ð»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð²ÐºÐ»Ð°Ð´ÐºÑƒ 'Summary' Ð´Ð»Ñ ÑÐµÑÑÐ¸Ð¸ {session_id}")
                raise e # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÑƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð½Ð° Ð±Ñ‹Ð»Ð° Ð¿Ð¾Ð¹Ð¼Ð°Ð½Ð° Ð½Ð¸Ð¶Ðµ

            # --- Ð’Ð°ÑˆÐ° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° ÑÐ±Ð¾Ñ€Ð° ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð¾Ð² ---
            screenshot_results = {}
            print("ðŸ“¸ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚Ð¾Ð²...")
            userinfo_path = self.screenshot_userinfo_block(page, session_id, temp_screenshots_dir)
            screenshot_results['userinfo'] = userinfo_path is not None
            if userinfo_path: screenshot_paths.append(userinfo_path)
            
            summary_paths = self.screenshot_summary_flexible(page, session_id, temp_screenshots_dir, summary_el=summary_el)
            screenshot_results['summary'] = len(summary_paths) > 0
            if summary_paths: screenshot_paths.extend(summary_paths)
            
            sentiment_path = self.screenshot_by_title(page, "Sentiment", session_id, temp_screenshots_dir)
            screenshot_results['sentiment'] = sentiment_path is not None
            if sentiment_path: screenshot_paths.append(sentiment_path)
            
            actions_path = self.screenshot_by_title(page, "Actions", session_id, temp_screenshots_dir)
            screenshot_results['actions'] = actions_path is not None
            if actions_path: screenshot_paths.append(actions_path)

            all_success = all(screenshot_results.get(block, False) for block in REQUIRED_BLOCKS)
            if not all_success or len(screenshot_paths) < 3:
                print(f"âŒ ÐÐµ ÑÐ¾Ð±Ñ€Ð°Ð½Ñ‹ Ð²ÑÐµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð±Ð»Ð¾ÐºÐ¸ Ð´Ð»Ñ {session_id}")
                return False, screenshot_paths
            
            session_dir, _ = self.create_session_folder_structure(session_id, screenshot_paths, url_data)
            uploaded_file = self.create_and_upload_session_archive(session_dir, session_id)
            return bool(uploaded_file), screenshot_paths
        
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ URL {url}: {e}")
            failure_path = os.path.join(temp_screenshots_dir, f"FAILURE_screenshot.png")
            try:
                page.screenshot(path=failure_path, full_page=True, timeout=15000)
                print(f"    Ð¡ÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½.")
            except Exception as e_scr:
                print(f"    ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ Ð¾ÑˆÐ¸Ð±ÐºÐ¸: {e_scr}")
            self.create_and_upload_session_archive(temp_screenshots_dir, session_id, is_failure=True)
            return False, []
        finally:
            shutil.rmtree(temp_screenshots_dir, ignore_errors=True)
            
    def process_batch(self, urls_batch):
        batch_start_time = time.time()
        batch_successful, batch_failed, batch_timeouts = 0, 0, 0
        self._update_status(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð±Ð°Ñ‚Ñ‡Ð° Ð¸Ð· {len(urls_batch)} URL...", -1)
        result_queue = multiprocessing.Queue()
        collector_config = {
            "credentials_path": self.credentials_path, "gdrive_folder_id": self.gdrive_folder_id,
            "bq_project_id": self.bq_project_id, "bq_dataset_id": self.bq_dataset_id,
            "bq_table_id": self.bq_table_id, "min_duration_seconds": self.min_duration_seconds,
            "cookies_path": self.cookies_path 
        }
        for i, url_data in enumerate(urls_batch, 1):
            self._update_status(f"â–¶ï¸ [{i}/{len(urls_batch)}] Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ URL ...{url_data['url'][-40:]}", -1)
            process = multiprocessing.Process(target=worker_process_url, args=(collector_config, url_data, result_queue))
            process.start()
            process.join(timeout=PROCESS_TIMEOUT_PER_URL)
            if process.is_alive():
                try:
                    self._update_status(f"â— Ð¢ÐÐ™ÐœÐÐ£Ð¢! ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ URL ...{url_data['url'][-40:]} Ð·Ð°Ð²Ð¸Ñ. Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼.", -1)
                    process.terminate(); process.join(timeout=5)
                    if process.is_alive(): process.kill(); process.join(timeout=5)
                    batch_timeouts += 1; batch_failed += 1
                    self.mark_url_as_processed(url_data['url'], success=False)
                except Exception as e:
                    self._update_status(f"âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ð°! {e}", -1)
                    batch_failed += 1; batch_timeouts += 1
            else:
                try:
                    success = result_queue.get_nowait()
                    if success: batch_successful += 1
                    else: batch_failed += 1
                except queue.Empty:
                    batch_failed += 1; self.mark_url_as_processed(url_data['url'], success=False)
            if i < len(urls_batch):
                time.sleep(random.uniform(2, 5))
        self.total_processed += len(urls_batch)
        self.total_successful += batch_successful
        self.total_failed += batch_failed
        self.total_timeouts += batch_timeouts
        self.batches_completed += 1
        batch_time = time.time() - batch_start_time
        self._update_status(f"ðŸ“¦ Ð‘Ð°Ñ‚Ñ‡ #{self.batches_completed} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð·Ð° {batch_time/60:.1f} Ð¼Ð¸Ð½. [Ð£ÑÐ¿ÐµÑˆÐ½Ð¾: {batch_successful}, ÐžÑˆÐ¸Ð±Ð¾Ðº: {batch_failed}, Ð—Ð°Ð²Ð¸ÑÐ°Ð½Ð¸Ð¹: {batch_timeouts}]", -1)
        
    def run(self):
        self.start_time = time.time()
        self._update_status("ðŸ”„ Ð—ÐÐŸÐ£Ð¡Ðš ÐÐ•ÐŸÐ Ð•Ð Ð«Ð’ÐÐžÐ™ ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ˜", 10)
        cycle_number = 0
        try:
            while True:
                cycle_number += 1
                if self.check_runtime_limit(): break
                self._update_status(f"\nðŸ” Ð¦Ð˜ÐšÐ› #{cycle_number}: ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ URL...", -1)
                urls_batch = self.get_unprocessed_urls(limit=self.batch_size)
                if not urls_batch:
                    self._update_status("ðŸŽ‰ Ð’ÑÐµ URL Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹!", -1)
                    break
                self._update_status(f"ðŸ“‹ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(urls_batch)} URL Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸", -1)
                self.process_batch(urls_batch)
                if not self.get_unprocessed_urls(limit=1):
                    self._update_status("ðŸŽ¯ Ð’ÑÐµ URL Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹!", -1)
                    break
                pause_time = random.uniform(self.pause_between_batches, self.pause_between_batches + 60)
                self._update_status(f"â¸ï¸ ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸: {pause_time:.1f} ÑÐµÐº...", -1)
                time.sleep(pause_time)
        except KeyboardInterrupt:
            self._update_status("âš ï¸ ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ ÑÐ¸Ð³Ð½Ð°Ð» Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸.", -1)
        except Exception as e:
            self._update_status(f"âŒ ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}", -1)
            import traceback
            traceback.print_exc()
        self.print_overall_stats()

    def get_safety_settings(self): return {'name':'placeholder'} # Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ Ð´Ð»Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸
    def print_overall_stats(self): print("Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹...") # Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ Ð´Ð»Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸
    def check_runtime_limit(self): return False # Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ Ð´Ð»Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸

def main():
    if sys.platform != 'win32':
        multiprocessing.set_start_method('spawn', force=True)
    multiprocessing.freeze_support()
    try:
        collector = RenderScreenshotCollector()
        collector.run()
    except Exception as e:
        print(f"âŒ ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐµ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()