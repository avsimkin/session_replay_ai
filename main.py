from fastapi import FastAPI, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import schedule
import time
import threading
import logging
import os
from datetime import datetime
import pytz
from typing import Dict, Any

from app.endpoints import router, run_script_safe

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º
scheduler_running = True
moscow_tz = pytz.timezone("Europe/Moscow")

def run_daily_analytics_pipeline():
    """–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")
    
    pipeline_steps = [
        ("scripts/1_collect_links_put_gbq.py", "–°–±–æ—Ä Session Replay —Å—Å—ã–ª–æ–∫"),
        ("scripts/2_replay_ai_gbq.py", "–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤"),
        ("scripts/3_collect_replay_screens.py", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"),
        ("scripts/4_get_clasters_gbq.py", "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"),
        ("scripts/5_summarazing.py", "–°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏")
    ]
    
    results = []
    total_start = datetime.now(moscow_tz)
    
    for script_path, step_name in pipeline_steps:
        logger.info(f"üìù –í—ã–ø–æ–ª–Ω—è–µ–º —ç—Ç–∞–ø: {step_name}")
        step_start = datetime.now(moscow_tz)
        
        try:
            result = run_script_safe(script_path, step_name)
            result["step_name"] = step_name
            results.append(result)
            
            step_end = datetime.now(moscow_tz)
            step_duration = (step_end - step_start).total_seconds()
            
            if result["status"] == "success":
                logger.info(f"‚úÖ {step_name} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ {step_duration:.1f} —Å–µ–∫")
            else:
                logger.error(f"‚ùå {step_name} –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π: {result.get('message', 'Unknown error')}")
                # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
                break
                
        except Exception as e:
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —ç—Ç–∞–ø–µ {step_name}: {str(e)}")
            results.append({
                "status": "critical_error",
                "step_name": step_name,
                "error": str(e),
                "start_time": step_start.isoformat(),
                "end_time": datetime.now(moscow_tz).isoformat()
            })
            break
    
    total_end = datetime.now(moscow_tz)
    total_duration = (total_end - total_start).total_seconds()
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    successful_steps = len([r for r in results if r["status"] == "success"])
    total_steps = len(results)
    
    pipeline_result = {
        "pipeline_status": "completed" if successful_steps == len(pipeline_steps) else "failed",
        "total_duration_seconds": total_duration,
        "total_duration_minutes": total_duration / 60,
        "start_time": total_start.isoformat(),
        "end_time": total_end.isoformat(),
        "successful_steps": successful_steps,
        "total_steps": total_steps,
        "steps_results": results
    }
    
    logger.info(f"üèÅ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω: {successful_steps}/{len(pipeline_steps)} —ç—Ç–∞–ø–æ–≤ —É—Å–ø–µ—à–Ω–æ")
    logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_duration/60:.1f} –º–∏–Ω—É—Ç")
    
    return pipeline_result

def run_scheduler():
    """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∑–∞–¥–∞—á –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("‚è∞ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –∑–∞–ø—É—â–µ–Ω")
    
    while scheduler_running:
        try:
            schedule.run_pending()
            time.sleep(60)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ: {str(e)}")
            time.sleep(60)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 09:00 MSK
    schedule.every().day.at("06:00").do(run_daily_analytics_pipeline)  # 09:00 MSK = 06:00 UTC
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
    # schedule.every().day.at("18:00").do(lambda: run_script_safe("scripts/summarazing.py", "Evening Summary"))
    
    # –ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()
    
    logger.info("üöÄ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ")
    logger.info("üìÖ –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 09:00 MSK")
    
    yield
    
    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    global scheduler_running
    scheduler_running = False
    logger.info("üõë –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="üìä Analytics Scripts API",
    description="""
    ## üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤ Session Replay
    
    –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ Session Replay –¥–∞–Ω–Ω—ã—Ö –∏–∑ Amplitude —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
    - **BigQuery** –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    - **Playwright** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞  
    - **OpenAI** –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏
    - **Google Drive** –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    
    ### üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞–π–ø–ª–∞–π–Ω (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 09:00 MSK):
    1. –°–±–æ—Ä Session Replay —Å—Å—ã–ª–æ–∫ –∏–∑ BigQuery
    2. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é
    3. ML-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM
    
    ### üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:
    - –°—Ç–∞—Ç—É—Å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –∑–∞–¥–∞—á
    - –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """,
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–æ–≤ —Å —Ç–µ–≥–∞–º–∏
app.include_router(router, prefix="/api")

@app.get("/", tags=["üìç General"])
async def root():
    """üè† –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ API"""
    return {
        "service": "üìä Analytics Scripts API",
        "status": "üü¢ running",
        "version": "1.0.0",
        "description": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ Session Replay",
        "scheduler_active": scheduler_running,
        "scheduled_jobs": len(schedule.jobs),
        "current_time_utc": datetime.now().isoformat(),
        "current_time_msk": datetime.now(moscow_tz).isoformat(),
        "features": {
            "üîÑ automatic_pipeline": "–ï–∂–µ–¥–Ω–µ–≤–Ω–æ –≤ 09:00 MSK",
            "üîó collect_links": "BigQuery ‚Üí Session Replay URLs",
            "üì∏ screenshots": "Playwright ‚Üí Google Drive",
            "üß† clustering": "ML –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è",
            "üìù summarize": "OpenAI –æ—Ç—á–µ—Ç—ã"
        },
        "endpoints": {
            "üìã scripts_status": "/api/monitoring/scripts",
            "‚è∞ scheduler_status": "/scheduler/status",
            "üöÄ full_pipeline": "/api/pipeline/full",
            "üìÖ daily_pipeline": "/api/pipeline/daily",
            "üìñ documentation": "/docs"
        }
    }

@app.get("/health", tags=["üìç General"])
async def health_check():
    """üíä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "üü¢ healthy",
        "timestamp": datetime.now().isoformat(),
        "scheduler_running": scheduler_running,
        "environment": os.environ.get("ENVIRONMENT", "production"),
        "uptime": "Running since startup"
    }

@app.get("/scheduler/status", tags=["‚è∞ Scheduler"])
async def scheduler_status():
    """‚è∞ –°—Ç–∞—Ç—É—Å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è"""
    jobs_info = []
    
    for job in schedule.jobs:
        try:
            next_run = job.next_run
            if next_run:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
                next_run_msk = next_run.replace(tzinfo=pytz.UTC).astimezone(moscow_tz)
                next_run_str = next_run_msk.strftime("%Y-%m-%d %H:%M:%S MSK")
            else:
                next_run_str = "Not scheduled"
                
            jobs_info.append({
                "job_function": str(job.job_func.__name__),
                "interval": str(job.interval),
                "unit": str(job.unit),
                "next_run_utc": str(job.next_run) if job.next_run else None,
                "next_run_msk": next_run_str,
                "tags": list(job.tags) if job.tags else []
            })
        except Exception as e:
            jobs_info.append({
                "error": f"Error getting job info: {str(e)}"
            })
    
    return {
        "scheduler_running": f"{'üü¢' if scheduler_running else 'üî¥'} {scheduler_running}",
        "jobs_count": len(schedule.jobs),
        "jobs": jobs_info,
        "current_time_utc": datetime.now().isoformat(),
        "current_time_msk": datetime.now(moscow_tz).isoformat(),
        "next_auto_run": "09:00 MSK –µ–∂–µ–¥–Ω–µ–≤–Ω–æ"
    }

@app.post("/run/daily-pipeline", tags=["üîÑ Manual Operations"])
async def run_daily_pipeline_manual(background_tasks: BackgroundTasks):
    """üéØ –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    def execute_pipeline():
        logger.info("üéØ –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞")
        return run_daily_analytics_pipeline()
    
    background_tasks.add_task(execute_pipeline)
    
    return {
        "message": "–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–ø—É—â–µ–Ω –≤—Ä—É—á–Ω—É—é",
        "status": "queued", 
        "start_time": datetime.now().isoformat(),
        "estimated_duration_minutes": "10-30"
    }

@app.get("/logs", tags=["üìä Monitoring"])
async def get_recent_logs():
    """üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª–æ–≥–∞—Ö"""
    # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–µ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ Render Dashboard)
    return {
        "message": "–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ª–æ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Render Dashboard",
        "log_location": "Render Dashboard -> Service -> Logs",
        "note": "–í—Å–µ –ª–æ–≥–∏ –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π logger Python"
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)